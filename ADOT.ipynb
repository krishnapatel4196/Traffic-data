{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff54ace-399b-49a5-9b7f-92e5b04bae90",
   "metadata": {},
   "source": [
    "<h1>Traffic data extraction code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56ce6cb8-3def-49ce-a11d-994fa98157cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required library\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25536f1-1e41-4320-95ab-04ad96306bf2",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will check the data to ensure the correct header</span></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3854b-b074-40f3-a642-83f088360086",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2007-2009</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22b6bda9-fd99-45a6-85e6-bf5c5b71631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to /Users/kishupatel/Desktop/2007-2009_filtered.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_excel('/Users/kishupatel/Desktop/DesFert/ADOT/2007-2009-AADT-PUBLICATION.xlsx')\n",
    "\n",
    "# Specify the list of CNTLOCID values you want to extract\n",
    "cntlocid_values = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]  # Replace with your specific CNTLOCID values\n",
    "\n",
    "# Filter the data for the specified CNTLOCID values\n",
    "filtered_data = data[data['CNTLOCID'].isin(cntlocid_values)]\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_extract = ['CNTLOCID', 'ROUTE', 'START', 'END', 'AADT 2007', 'AADT 2008', 'AADT 2009']\n",
    "filtered_data = filtered_data[columns_to_extract]\n",
    "\n",
    "# Create the new file name by appending 'filtered' to the original name\n",
    "new_file = '/Users/kishupatel/Desktop/2007-2009_filtered.xlsx'\n",
    "\n",
    "# Save the filtered data to an Excel file\n",
    "filtered_data.to_excel(new_file, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {new_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a23ae-4f08-4b59-9aed-28d9425dcc4d",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2011</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "825c7f27-e373-4c20-8fda-56df4d470fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to /Users/kishupatel/Desktop/2011_filtered.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = ('/Users/kishupatel/Desktop/DesFert/ADOT/2011-AADT-PUBLICATION.xls')\n",
    "data = pd.read_excel(file_path, sheet_name='SHS Traffic Log 2011')\n",
    "# Specify the list of CNTLOCID values you want to extract\n",
    "cntlocid_values = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904] \n",
    "\n",
    "# Filter the data for the specified CNTLOCID values\n",
    "filtered_data = data[data['CNTLOCID'].isin(cntlocid_values)]\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_extract = ['CNTLOCID', 'ROUTE', 'START', 'END', 'AADT 2011']\n",
    "filtered_data = filtered_data[columns_to_extract]\n",
    "\n",
    "# Create the new file name by appending 'filtered' to the original name\n",
    "new_file = '/Users/kishupatel/Desktop/2011_filtered.xlsx'\n",
    "\n",
    "# Save the filtered data to an Excel file\n",
    "filtered_data.to_excel(new_file, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {new_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91be304-dfb9-41f3-9a09-f0741816de73",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2014</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1518db01-ebd6-4963-978e-5ce6a5883911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to /Users/kishupatel/Desktop/2014_filtered.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = ('/Users/kishupatel/Desktop/DesFert/ADOT/2014-AADT.xlsx')\n",
    "data = pd.read_excel(file_path, sheet_name='Table 1')\n",
    "# Specify the list of CNTLOCID values you want to extract\n",
    "cntlocid_values = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904] \n",
    "\n",
    "# Filter the data for the specified CNTLOCID values\n",
    "filtered_data = data[data['CNTLOCID'].isin(cntlocid_values)]\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_extract = ['CNTLOCID', 'ROUTE', 'START', 'END', 'AADT 2014']\n",
    "filtered_data = filtered_data[columns_to_extract]\n",
    "\n",
    "# Create the new file name by appending 'filtered' to the original name\n",
    "new_file = '/Users/kishupatel/Desktop/2014_filtered.xlsx'\n",
    "\n",
    "# Save the filtered data to an Excel file\n",
    "filtered_data.to_excel(new_file, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {new_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273aab3-3173-4f02-acc2-154dda0d901d",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2015</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53c9e4da-7ca2-4d9d-9baa-2a7c87b4c863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: State Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2015', 'AADT Derivation Code', 'K Factor', 'D Factor', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 AADT']\n",
      "\n",
      "üîπ Processing sheet: US Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2015', 'AADT Derivation Code', 'K Factor', 'D Factor', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 AADT']\n",
      "\n",
      "üîπ Processing sheet: Interstates\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2015', 'AADT Derivation Code', 'K Factor', 'D Factor', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 AADT']\n",
      "\n",
      "üîπ Processing sheet: AADT 2015\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2015', 'AADT Derivation Code', 'K Factor', 'D Factor', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 AADT']\n",
      "üîπ Filtering data from sheet: State Routes\n",
      "üîπ Filtering data from sheet: US Routes\n",
      "üîπ Filtering data from sheet: Interstates\n",
      "üîπ Filtering data from sheet: AADT 2015\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2015_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2015-AADT-Publication_0.xlsx'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Route', 'Start', 'End', 'AADT 2015']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878aa53-8851-44e1-92c2-3b199525c6d9",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2016</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d332a71-4868-4dd8-b324-6493fe6b1166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: AADT 2016\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2016', 'AADT Derivation Code', 'K Factor', 'D Factor', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 AADT']\n",
      "\n",
      "üîπ Processing sheet: INTERSTATES\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2016', 'AADT Derivation Code', 'K Factor', 'D Factor', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 AADT']\n",
      "\n",
      "üîπ Processing sheet: US ROUTES\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2016', 'AADT Derivation Code', 'K Factor', 'D Factor', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 AADT']\n",
      "\n",
      "üîπ Processing sheet: STATE ROUTES\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2016', 'AADT Derivation Code', 'K Factor', 'D Factor', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', 'Future AADT']\n",
      "üîπ Filtering data from sheet: AADT 2016\n",
      "üîπ Filtering data from sheet: INTERSTATES\n",
      "üîπ Filtering data from sheet: US ROUTES\n",
      "üîπ Filtering data from sheet: STATE ROUTES\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2016_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2016-aadt-Publication_1.xlsx'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Route', 'Start', 'End', 'AADT 2016']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5144eb3-0db1-4456-9370-231d61452915",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2017</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7f3a5c4-1e2d-40db-b2f8-0a247b374045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: 2017 AADTS\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2017', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: STATE ROUTES\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2017', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: US ROUTES\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2017', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035  Future AADT']\n",
      "\n",
      "üîπ Processing sheet: INTERSTATES\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2017', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2035 Future AADT']\n",
      "üîπ Filtering data from sheet: 2017 AADTS\n",
      "üîπ Filtering data from sheet: STATE ROUTES\n",
      "üîπ Filtering data from sheet: US ROUTES\n",
      "üîπ Filtering data from sheet: INTERSTATES\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2017_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2017-aadt-Publication.xlsm'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Route', 'Start', 'End', 'AADT 2017']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4a8c5-4bf8-4f96-b310-e18f3cbd5b52",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2018</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fda1c2a-8c15-40a8-9f1f-0a1b5f7f5afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: Arizona Interstate Sections\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2018', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Motorcyles', 'AADT Passenger Cars', 'AADT Light Trucks', 'AADT Buses', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona State Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2018', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Motorcyles', 'AADT Passenger Cars', 'AADT Light Trucks', 'AADT Buses', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: ALL MAINLINE SECTIONS\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2018', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Motorcyles', 'AADT Passenger Cars', 'AADT Light Trucks', 'AADT Buses', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona US Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2018', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Motorcyles', 'AADT Passenger Cars', 'AADT Light Trucks', 'AADT Buses', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "üîπ Filtering data from sheet: Arizona Interstate Sections\n",
      "üîπ Filtering data from sheet: Arizona State Routes\n",
      "üîπ Filtering data from sheet: ALL MAINLINE SECTIONS\n",
      "üîπ Filtering data from sheet: Arizona US Routes\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2018_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2018-AADT-PUBLICATION.xlsx'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Route', 'Start', 'End', 'AADT 2018']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d807d2f-058b-4f15-8be9-81e76c5877f8",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2019</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d42bbbc-8791-4b67-855c-d49252d5f186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: ALL MAINLINE SECTIONS\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2019', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona Interstate Sections\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2019', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona State Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2019', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona US Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2019', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "üîπ Filtering data from sheet: ALL MAINLINE SECTIONS\n",
      "üîπ Filtering data from sheet: Arizona Interstate Sections\n",
      "üîπ Filtering data from sheet: Arizona State Routes\n",
      "üîπ Filtering data from sheet: Arizona US Routes\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2019_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2019-AADT-PUBLICATION.xlsx'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Route', 'Start', 'End', 'AADT 2019']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35e56e-ada6-4f20-975d-c30448383f2a",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2020</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9df692cb-db65-4623-995c-28c1f72c3986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: ALL MAINLINE SECTIONS\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2020', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona Interstate Sections\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2020', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona State Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2020', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona US Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2020', 'AADT Derivation Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "üîπ Filtering data from sheet: ALL MAINLINE SECTIONS\n",
      "üîπ Filtering data from sheet: Arizona Interstate Sections\n",
      "üîπ Filtering data from sheet: Arizona State Routes\n",
      "üîπ Filtering data from sheet: Arizona US Routes\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2020_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2020-AADT-PUBLICATION.xlsx'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Route', 'Start', 'End', 'AADT 2020']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f379dfc-dda5-4989-9702-c9bf91f31204",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2021</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94432b57-1585-4cb5-bc1b-4bcfd403103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: ALL MAINLINE SECTIONS\n",
      "‚úÖ Finalized columns: ['Index', 'Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2021', 'AADT Source Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona Interstate Sections\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2021', 'AADT Souce Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona State Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2021', 'AADT Souce Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "\n",
      "üîπ Processing sheet: Arizona US Routes\n",
      "‚úÖ Finalized columns: ['Loc ID', 'Route', 'BMP', 'Start', 'TCS MP', 'EMP', 'End', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2021', 'AADT Souce Code', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2040 Future AADT']\n",
      "üîπ Filtering data from sheet: ALL MAINLINE SECTIONS\n",
      "üîπ Filtering data from sheet: Arizona Interstate Sections\n",
      "üîπ Filtering data from sheet: Arizona State Routes\n",
      "üîπ Filtering data from sheet: Arizona US Routes\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2021_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2021-AADT-PUBLICATION_20220615.xlsx'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Route', 'Start', 'End', 'AADT 2021']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf41c4-6f70-4276-992d-2dd3900674f8",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2022</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73621021-644c-4cd4-b835-3a65ba8e152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: 2022 ALL ADOT SECTIONS\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'FromMeasure', 'BMP', 'FromRoad', 'TCS MP', 'ToMeasure', 'EMP', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2022', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2042 Future AADT', 'Index', 'nan', '2035', 'nan', 'CheckMP']\n",
      "\n",
      "üîπ Processing sheet: Arizona Interstate Sections\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'BMP', 'FromRoad', 'TCS MP', 'EMP', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2022', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2042 Future AADT', 'nan', '2031']\n",
      "\n",
      "üîπ Processing sheet: Arizona State Routes\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'BMP', 'FromRoad', 'TCS MP', 'EMP', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2022', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2042 Future AADT', 'Index', 'nan', '2042']\n",
      "\n",
      "üîπ Processing sheet: Arizona US Routes\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'BMP', 'FromRoad', 'TCS MP', 'EMP', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2022', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2042 Future AADT', 'Index', 'nan', '2035']\n",
      "\n",
      "üîπ Processing sheet: Ramps and Frontage Roads\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'FromMeasure', 'FromRoad', 'ToMeasure', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2022', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2042 Future AADT', 'nan', '2042']\n",
      "üîπ Filtering data from sheet: 2022 ALL ADOT SECTIONS\n",
      "üîπ Filtering data from sheet: Arizona Interstate Sections\n",
      "üîπ Filtering data from sheet: Arizona State Routes\n",
      "üîπ Filtering data from sheet: Arizona US Routes\n",
      "üîπ Filtering data from sheet: Ramps and Frontage Roads\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2022_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2022-AADT-PUBLICATION_20230825.xlsx'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Road', 'FromRoad', 'ToRoad', 'AADT 2022']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7a8c4-88fd-45d6-af07-9623cbfcd3fb",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:Blue;\">This code will extract traffic data for the year 2023</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dadd95f2-58c8-4aca-8b8b-783fe178ad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing sheet: 2023 ALL ADOT SECTIONS\n",
      "‚úÖ Finalized columns: ['Section JoinID', 'Reference', 'Loc ID', 'Traffic Section Type', 'RouteId', 'Miles', 'Road', 'From Measure', 'BMP', 'FromRoad', 'TCS MP', 'To Measure', 'EMP', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2023', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2043 Future AADT', 'Type', 'Index']\n",
      "\n",
      "üîπ Processing sheet: Arizona Interstate Sections\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'BMP', 'FromRoad', 'TCS MP', 'EMP', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2023', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2043 Future AADT', 'Index']\n",
      "\n",
      "üîπ Processing sheet: Arizona State Routes\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'BMP', 'FromRoad', 'TCS MP', 'EMP', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2023', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2043 Future AADT', 'nan']\n",
      "\n",
      "üîπ Processing sheet: Arizona US Routes\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'BMP', 'FromRoad', 'TCS MP', 'EMP', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2023', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2043 Future AADT', 'nan']\n",
      "\n",
      "üîπ Processing sheet: Ramps and Frontage Roads\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'TrafficSectionType', 'RouteId', 'Miles', 'Road', 'From Measure', 'FromRoad', 'To Measure', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2023', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2043 Future AADT', 'nan', 'Index']\n",
      "\n",
      "üîπ Processing sheet: 2023 LOCAL SECTIONS\n",
      "‚úÖ Finalized columns: ['SectionJoinID', 'Reference', 'Loc ID', 'Traffic Section Type', 'RouteId', 'Miles', 'Road', 'From Measure', 'FromRoad', 'To Measure', 'ToRoad', 'Pos Dir AADT', 'Neg Dir AADT', 'AADT 2023', 'AADT Source Dataset', 'K Factor %', 'D Factor %', 'AADT Single Trucks', 'AADT Combo Trucks', 'T Factor %', '2043 Future AADT']\n",
      "üîπ Filtering data from sheet: 2023 ALL ADOT SECTIONS\n",
      "üîπ Filtering data from sheet: Arizona Interstate Sections\n",
      "üîπ Filtering data from sheet: Arizona State Routes\n",
      "üîπ Filtering data from sheet: Arizona US Routes\n",
      "üîπ Filtering data from sheet: Ramps and Frontage Roads\n",
      "üîπ Filtering data from sheet: 2023 LOCAL SECTIONS\n",
      "Filtered data saved to: /Users/kishupatel/Desktop/2023_filtered_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "file_path = '/Users/kishupatel/Desktop/DesFert/ADOT/2023-AADT-PUBLICATION_20240704.xlsx'\n",
    "dfs = pd.read_excel(file_path, sheet_name=None, header=None, dtype=str)  # Read all sheets without assuming a header\n",
    "\n",
    "cleaned_sheets = {}\n",
    "\n",
    "for sheet_name, df in dfs.items():\n",
    "    print(f\"\\nüîπ Processing sheet: {sheet_name}\")\n",
    "    # Drop fully empty rows (entirely NaN)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Find the first row with valid data (assumes it has at least half non-null values)\n",
    "    for i in range(len(df)):\n",
    "        non_null_count = df.iloc[i].notna().sum()\n",
    "        if non_null_count > len(df.columns) / 2:\n",
    "            valid_header_row = i\n",
    "            break\n",
    "\n",
    "    # Set detected row as header\n",
    "    df.columns = df.iloc[valid_header_row].astype(str).str.strip()  # Remove spaces from column names\n",
    "    # Drop all rows above the detected header row\n",
    "    df = df.iloc[valid_header_row + 1:].reset_index(drop=True)\n",
    "    # Remove duplicate header rows appearing later\n",
    "    df = df[df['Loc ID'] != 'Loc ID']\n",
    "    # Print detected column names for debugging\n",
    "    print(f\"‚úÖ Finalized columns: {df.columns.tolist()}\")\n",
    "    # Ensure 'Loc ID' exists before proceeding\n",
    "    if 'Loc ID' in df.columns:\n",
    "        df = df[df['Loc ID'].notna()]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "        continue  # Skip if 'Loc ID' is missing\n",
    "    # Convert 'Loc ID' to numeric\n",
    "    df['Loc ID'] = pd.to_numeric(df['Loc ID'], errors='coerce')\n",
    "    # Store cleaned sheet\n",
    "    cleaned_sheets[sheet_name] = df\n",
    "# -------------------------- #\n",
    "# üîπ Now Filter and Combine Data #\n",
    "# -------------------------- #\n",
    "# Specify the Loc IDs to retrieve\n",
    "given_ids = [100022, 100023, 100085, 100086, 100088, 100100, 100101, 100102, 100112, 100113, 100117, 100348, 100677,\n",
    "             100678, 100679, 100918, 100980, 100982, 101240, 101385, 101597, 101598, 101599, 101600, 101890, 101891,\n",
    "             101902, 101903, 101904]\n",
    "# Specify the columns to retrieve\n",
    "columns_to_select = ['Loc ID', 'Road', 'FromRoad', 'ToRoad', 'AADT 2023']\n",
    "all_filtered_data = []\n",
    "\n",
    "# Process cleaned sheets\n",
    "for sheet_name, df in cleaned_sheets.items():\n",
    "    print(f\"üîπ Filtering data from sheet: {sheet_name}\")\n",
    "    # Ensure 'Loc ID' exists before filtering\n",
    "    if 'Loc ID' in df.columns:\n",
    "        filtered_data = df[df['Loc ID'].isin(given_ids)][columns_to_select]\n",
    "        if not filtered_data.empty:\n",
    "            all_filtered_data.append(filtered_data)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: 'Loc ID' not found in sheet '{sheet_name}'. Skipping this sheet.\")\n",
    "\n",
    "# Combine all filtered data\n",
    "if all_filtered_data:\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True).drop_duplicates()\n",
    "    # Get the first 4 letters of the original file name (without extension)\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0][:4]\n",
    "    # Create output file path with the first 4 letters + filtered data\n",
    "    output_file_path = f'/Users/kishupatel/Desktop/{file_name}_filtered_data.xlsx'\n",
    "    # Save final output\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='CombinedData', index=False)\n",
    "    print(f\"Filtered data saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No matching data found for the specified Loc IDs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
